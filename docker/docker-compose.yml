services:
  redis:
    image: redis
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - $HOME/.redis:/data
    command: ["redis-server"]
    networks:
      - spark-net

  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: redis-commander
    environment:
      - REDIS_HOSTS=local:redis:6379
    ports:
      - "8082:8081"
    depends_on:
      - redis
    networks:
      - spark-net

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    volumes:
      - ../app:/app
      - ../jobs:/opt/airflow/jobs
    ports:
      - "8085:8080"
      - "7077:7077"
    networks:
      - spark-net

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ../app:/app
      - ../jobs:/opt/airflow/jobs
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    command:
      - bash
      - -c
      - |
        pip install -r /app/requirements.txt && \
        /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh
    networks:
      - spark-net

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - spark-net

  airflow_redis:
    image: redis:7
    ports:
      - "6380:6379"
    networks:
      - spark-net

  airflow-webserver:
    container_name: airflow-webserver
    build:
      context: .
      dockerfile: Dockerfile-airflow
    depends_on:
      - postgres
      - airflow_redis
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: 'SUPER_SECRET_KEY'
      AIRFLOW__CORE__FERNET_KEY: BDV4hA56l2t8BdkDXDWJooAWQdKj4xwBHWJLoAKNQuw=
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    volumes:
      - ../dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - ../jobs:/opt/airflow/jobs
      # - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: >
      bash -c "
        airflow db upgrade &&
        airflow users create --username admin --firstname test --lastname user --role Admin --email admin@example.com --password admin &&
        airflow webserver
      "
    restart: unless-stopped
    networks:
      - spark-net

  airflow-scheduler:
    container_name: airflow-scheduler
    build:
      context: .
      dockerfile: Dockerfile-airflow
    depends_on:
      - airflow-webserver
      - postgres
      - airflow_redis
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: 'SUPER_SECRET_KEY'
      AIRFLOW__CORE__FERNET_KEY: BDV4hA56l2t8BdkDXDWJooAWQdKj4xwBHWJLoAKNQuw=
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    volumes:
      - ../dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - ../jobs:/opt/airflow/jobs
      # - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    restart: unless-stopped
    networks:
      - spark-net

volumes:
  postgres-db-volume:
  airflow-logs:
networks:
  spark-net: